{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyN82EJM9q2Lpe+axwoS/w8W"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"PfuqkertHWxi","executionInfo":{"status":"ok","timestamp":1728979309129,"user_tz":-540,"elapsed":3774,"user":{"displayName":"Jinyeong Wang","userId":"15748183116968670585"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import DataLoader, Dataset\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["class TextDataset(Dataset):\n","    def __init__(self, texts, labels, tokenizer, max_len=50):\n","        self.texts = texts\n","        self.labels = labels\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","    def __len__(self):\n","        return len(self.texts)\n","    def __getitem__(self, idx):\n","        text = self.texts[idx]\n","        label = self.labels[idx]\n","        inputs = self.tokenizer(text, return_tensors='pt', max_length=self.max_len, padding='max_length', truncation=True)\n","        input_ids = inputs['input_ids'].squeeze(0)\n","        attention_mask = inputs['attention_mask'].squeeze(0)\n","        attention_maxk = attention_mask.bool()\n","        return input_ids, attention_maxk, torch.tensor(label)"],"metadata":{"id":"go2OJ1PpHkf8","executionInfo":{"status":"ok","timestamp":1728979309129,"user_tz":-540,"elapsed":5,"user":{"displayName":"Jinyeong Wang","userId":"15748183116968670585"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["class TransformerModel(nn.Module):\n","    def __init__(self, vocab_size, embed_size, num_classes, num_heads, num_layers, max_len):\n","        super().__init__()\n","        self.embedding = nn.Embedding(vocab_size, embed_size)\n","        self.pos_encoder = nn.Embedding(max_len, embed_size)\n","        self.transformer = nn.Transformer(d_model=embed_size, nhead=num_heads, num_encoder_layers=num_layers)\n","        self.fc = nn.Linear(embed_size, num_classes)\n","    def forward(self, input_ids, attention_mask):\n","        seq_len = input_ids.size(1)\n","        pos = torch.arange(0, seq_len).unsqueeze(0).to(input_ids.device)\n","        x = self.embedding(input_ids) + self.pos_encoder(pos)\n","        x = x.transpose(0, 1)\n","        x = self.transformer(x, x, src_key_padding_mask=attention_mask)\n","        x = x.mean(dim=0)\n","        return self.fc(x)"],"metadata":{"id":"Ub06zyRhIRRx","executionInfo":{"status":"ok","timestamp":1728979309129,"user_tz":-540,"elapsed":4,"user":{"displayName":"Jinyeong Wang","userId":"15748183116968670585"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["vocab_size = 30522\n","embed_size = 128\n","num_classes = 2\n","num_heads = 8\n","num_layers = 2\n","max_len = 50"],"metadata":{"id":"hJTVknVSI6tk","executionInfo":{"status":"ok","timestamp":1728979309130,"user_tz":-540,"elapsed":5,"user":{"displayName":"Jinyeong Wang","userId":"15748183116968670585"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!pip install sacremoses"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0D5CS5UJ8Qf","executionInfo":{"status":"ok","timestamp":1728979314238,"user_tz":-540,"elapsed":5113,"user":{"displayName":"Jinyeong Wang","userId":"15748183116968670585"}},"outputId":"c7d9d158-9741-48bd-cfb8-d52998725a6b"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2024.9.11)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.4.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.5)\n"]}]},{"cell_type":"code","source":["texts = [\"I love programming\",\n","         \"This is a great day\",\n","         \"I hate bugs\",\n","         \"Debugging is fun\",\n","         \"I love this movie, it was fantastic!\",\n","         \"This is the worst film I have ever seen.\",\n","         \"I am so happy with the service.\",\n","         \"I hate waiting in line for so long.\",\n","         \"The food was great, I will come again.\",\n","         \"It was a terrible experience, I will not return.\",\n","         \"Amazing product, highly recommend it.\",\n","         \"The staff was very rude and unhelpful.\",\n","         \"I am extremely satisfied with my purchase.\",\n","         \"This place is awful, never coming back.\"\n","         ]\n","labels = [1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]\n","tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'bert-base-uncased')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TiJgqR-IJAf0","executionInfo":{"status":"ok","timestamp":1728979317606,"user_tz":-540,"elapsed":3371,"user":{"displayName":"Jinyeong Wang","userId":"15748183116968670585"}},"outputId":"4159a89a-60de-46d1-8ec5-49cbd9ca23b6"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/huggingface_pytorch-transformers_main\n","/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n","train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n","val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n","train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n","val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)"],"metadata":{"id":"kF6s8ayVJ7Xm","executionInfo":{"status":"ok","timestamp":1728979317607,"user_tz":-540,"elapsed":5,"user":{"displayName":"Jinyeong Wang","userId":"15748183116968670585"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n","model = TransformerModel(vocab_size, embed_size, num_classes, num_heads, num_layers, max_len).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.0001)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WJYt1ZFsJGkn","executionInfo":{"status":"ok","timestamp":1728979319338,"user_tz":-540,"elapsed":1736,"user":{"displayName":"Jinyeong Wang","userId":"15748183116968670585"}},"outputId":"458276ed-0261-4ce6-f36b-f2ae5852c6ce"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"]}]},{"cell_type":"code","source":["num_epochs = 50\n","for epoch in range(num_epochs):\n","    model.train()\n","    total_loss = 0\n","    for input_ids, attention_mask, labels in train_loader:\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(train_loader)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AhfNnkDwKeaR","executionInfo":{"status":"ok","timestamp":1728979323410,"user_tz":-540,"elapsed":4075,"user":{"displayName":"Jinyeong Wang","userId":"15748183116968670585"}},"outputId":"93d85009-c27c-4710-bdae-711a8cbf2ae9"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50, Loss: 0.7166368961334229\n","Epoch 2/50, Loss: 1.4781361818313599\n","Epoch 3/50, Loss: 0.8576106429100037\n","Epoch 4/50, Loss: 0.6823650002479553\n","Epoch 5/50, Loss: 0.8599671721458435\n","Epoch 6/50, Loss: 0.8127077221870422\n","Epoch 7/50, Loss: 0.7237362861633301\n","Epoch 8/50, Loss: 0.6875954270362854\n","Epoch 9/50, Loss: 0.6774516701698303\n","Epoch 10/50, Loss: 0.7043421268463135\n","Epoch 11/50, Loss: 0.7310829162597656\n","Epoch 12/50, Loss: 0.7214116454124451\n","Epoch 13/50, Loss: 0.7167315483093262\n","Epoch 14/50, Loss: 0.6972237229347229\n","Epoch 15/50, Loss: 0.682255744934082\n","Epoch 16/50, Loss: 0.6754511594772339\n","Epoch 17/50, Loss: 0.6774863600730896\n","Epoch 18/50, Loss: 0.6777464151382446\n","Epoch 19/50, Loss: 0.6912023425102234\n","Epoch 20/50, Loss: 0.6986187100410461\n","Epoch 21/50, Loss: 0.6872559785842896\n","Epoch 22/50, Loss: 0.6701497435569763\n","Epoch 23/50, Loss: 0.6718823909759521\n","Epoch 24/50, Loss: 0.6656598448753357\n","Epoch 25/50, Loss: 0.6629288792610168\n","Epoch 26/50, Loss: 0.6604506969451904\n","Epoch 27/50, Loss: 0.6740594506263733\n","Epoch 28/50, Loss: 0.6588523387908936\n","Epoch 29/50, Loss: 0.6655745506286621\n","Epoch 30/50, Loss: 0.6559973359107971\n","Epoch 31/50, Loss: 0.6506364345550537\n","Epoch 32/50, Loss: 0.6486929059028625\n","Epoch 33/50, Loss: 0.6407045722007751\n","Epoch 34/50, Loss: 0.6428179144859314\n","Epoch 35/50, Loss: 0.6355845928192139\n","Epoch 36/50, Loss: 0.6349114179611206\n","Epoch 37/50, Loss: 0.6404218077659607\n","Epoch 38/50, Loss: 0.6144041419029236\n","Epoch 39/50, Loss: 0.6023086905479431\n","Epoch 40/50, Loss: 0.5997070670127869\n","Epoch 41/50, Loss: 0.5689725279808044\n","Epoch 42/50, Loss: 0.56374192237854\n","Epoch 43/50, Loss: 0.5496495962142944\n","Epoch 44/50, Loss: 0.5156497359275818\n","Epoch 45/50, Loss: 0.47419804334640503\n","Epoch 46/50, Loss: 0.4411018192768097\n","Epoch 47/50, Loss: 0.41501855850219727\n","Epoch 48/50, Loss: 0.36133769154548645\n","Epoch 49/50, Loss: 0.35034188628196716\n","Epoch 50/50, Loss: 0.282767117023468\n"]}]},{"cell_type":"code","source":["model.eval()\n","correct = 0\n","total = 0\n","with torch.no_grad():\n","    for input_ids, attention_mask, labels in val_loader:\n","        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n","        outputs = model(input_ids, attention_mask)\n","        _, predicted = torch.max(outputs, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","print(f\"Validation Accuracy: {correct / total * 100:.2f}%\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"x-it54KYLDmD","executionInfo":{"status":"ok","timestamp":1728979323410,"user_tz":-540,"elapsed":7,"user":{"displayName":"Jinyeong Wang","userId":"15748183116968670585"}},"outputId":"beb728f1-df25-414f-f164-bfadb39e9648"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Validation Accuracy: 66.67%\n"]}]},{"cell_type":"code","source":["def predict(text, model, tokenizer, max_len=50):\n","    model.eval()\n","    with torch.no_grad():\n","        inputs = tokenizer(text, return_tensors='pt', max_length=max_len, padding='max_length', truncation=True)\n","        input_ids = inputs['input_ids'].to(device)\n","        attention_mask = inputs['attention_mask'].to(device)\n","        attention_mask = attention_mask.bool()\n","        output = model(input_ids, attention_mask)\n","        _, predicted = torch.max(output, 1)\n","    return predicted.item()"],"metadata":{"id":"0KSydKGMLvCz","executionInfo":{"status":"ok","timestamp":1728979323410,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jinyeong Wang","userId":"15748183116968670585"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["input_text = \"I love it\"\n","predicted_class = predict(input_text, model, tokenizer, max_len)\n","print(f\"Predicted Class: {predicted_class}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RaQgziVaMaSA","executionInfo":{"status":"ok","timestamp":1728979323411,"user_tz":-540,"elapsed":6,"user":{"displayName":"Jinyeong Wang","userId":"15748183116968670585"}},"outputId":"322594c4-c0b3-46f9-87ff-8580eec4eda7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Predicted Class: 1\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"ZMt8W868MhXA","executionInfo":{"status":"ok","timestamp":1728979323411,"user_tz":-540,"elapsed":5,"user":{"displayName":"Jinyeong Wang","userId":"15748183116968670585"}}},"execution_count":12,"outputs":[]}]}